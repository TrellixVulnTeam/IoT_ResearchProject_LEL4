---
title: 'A Model for Energy-Saving in an IoT Smarthome accounting for End-User Convenience'
author: 'Alistair Francis Bowman Grevis-James'
date: 'November 2019'
institution: 'Royal Melbourne Institute of Technology'
division: 'Computer Sciences'
advisor: 'Dr Hai Dong'
abstract: |
  `r if(knitr:::is_latex_output()) paste(readLines("00-abstract.Rmd"), collapse = '\n  ')` \pagebreak
acknowledgements: |
  I want to thank a few people.
dedication: |
  You can have a dedication here if you wish. 
preface: |
  This is an example of a thesis setup to use the reed thesis document class
  (for LaTeX) and the R bookdown package, in general.
output: 
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    fig_crop: true
    fig_caption: yes
    keep_tex: yes
    highlight: "kate"
bibliography: biblio.bib
csl: computer-science-research-and-development.csl
link-citations: yes
fontsize: 11pt
geometry: margin=0.79in
header-includes:
 \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
 \SetAlFnt{\tiny}
 \SetAlCapNameFnt{\small}
 \usepackage{booktabs}
 \usepackage{longtable}
 \usepackage{array}
 \usepackage{multirow}
 \usepackage[table]{xcolor}
 \usepackage{wrapfig}
 \usepackage{float}
 \floatplacement{figure}{H}
 \usepackage{caption, setspace}
 \captionsetup[figure]{font={stretch=1,scriptsize}}
 \captionsetup[figure]{skip=2pt}
 \captionsetup[table]{font={stretch=1,scriptsize}}
 \captionsetup[table]{skip=2pt}
---

```{r Rpackages, message=FALSE, warning=FALSE, include=FALSE}
# all your necessary packages
packages <- c("countrycode",  "cowplot",  "bibtex", "dplyr", "ggExtra", "tidyverse",
              "ggplot2",  "ggridges", "kableExtra", "knitr", "lubridate", "readr",
             "tidyr", "viridis", "reticulate", "ggsci", "scales", "bibtex") 
# install if needed and loading packages
to_install <- packages[! packages %in% installed.packages()[, "Package"]]
if (length(to_install)) { 
  install.packages(to_install, repos = "https://cloud.r-project.org")
}
invisible(lapply(packages, library, character.only = TRUE))
# get the packages version 
packages_versions <- function(p) {
  paste(packageDescription(p)$Package, packageDescription(p)$Version, sep = " ")
}

# Get the packages references
write.bib(packages, "packages.bib")

# merge the zotero references and the packages references
cat(paste("% Automatically generated", Sys.time()), "\n% DO NOT EDIT",
    { readLines("zotero.bib") %>% 
      paste(collapse = "\n") },
    { readLines("packages.bib") %>% 
      paste(collapse = "\n") },
    file = "biblio.bib",
    sep = "\n")
# Some packages reference keys must be modified
# (their key is not the package name)
# check in packages.bib
packages_keys <- packages %>% enframe() %>% 
  mutate(value = case_when(value == "knitr" ~ "@knitr1",
                           #value == "boot" ~ "@boot1",
                           TRUE ~ paste0("@", value)))
use_python("/anaconda3/envs/IoT_ResearchProject/bin/python", required = T)
```

```{python PythonPackages, include=FALSE}
import numpy as np
import pandas as pd
import json
from tabulate import tabulate
from more_itertools import chunked   
PATH = '/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November'
```

```{r global_options, include=FALSE}
knitr::opts_chunk$set(
  fig.align='center',
  fig.pos = 'H',
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

\pagebreak

# Introduction

## Background

### Humankind, Technology & Development 

Since the inception of the first home computers in the late 1970s [@pressAltairHistoryPersonal1993], modern society has become dependent on, and - indeed - inexorably bound to digital technology. The rapid and widespread adoption of computational technology has led to the fastest rate of societal and economic development our species has ever experienced, as exemplified in Figure \ref{fig:satelliteImagesAsia} and Figure \ref{fig:globalPovertyPlot}, below. One of the most salient manifestations of progress has been the widespread availability and adoption of Information and Communications Technology (**ICT**), including the rise of the global network of networks known as the Internet.

```{r satelliteImagesAsia, echo=FALSE, fig.cap="Satellite images of South Asia by night. Left (South Asia in 1994) Right (South Asia in 2010). Images are taken from Maxim Pinkovskiy and Xavier Sala-i-Martin (2016) - Lights, Camera ... Income! Illuminating the National Accounts-Household Surveys Debate. The Quarterly Journal of Economics.", fig.align='center', out.width = '60%'}
knit_hooks$set(crop = hook_pdfcrop)
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/satelliteImages.png')
```
```{r globalPovertyPreprocessing, message=FALSE, warning=FALSE, include=FALSE}
poverty <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/datasets/total-population-living-in-extreme-poverty-by-world-region.csv')
colnames(poverty)[4] = 'Number'
poverty <- poverty %>% filter(Entity != 'World')
```

```{r globalPovertyPlot, echo=FALSE, fig.align='center', fig.cap="The number of people living in extreme poverty between the years of 1990 till 2015, segmented by region. Source: The World Bank", fig.height=2.5, fig.width=6, message=FALSE, warning=FALSE}
p <- ggplot(poverty, aes(x = Year, y = Number, fill = Entity))
p <- p + geom_area(alpha=0.9 , size=.2, colour="white") +
  scale_x_continuous(limits = c(1990, 2015)) + 
  scale_x_continuous(breaks = c(1990, 1995, 2000, 2005, 2010, 2015), expand = c(0.02, 0.02)) +
  theme_grey() +
  scale_fill_viridis(discrete = T) +
  labs(title="Number of people living in extreme poverty",
       y="Number of People") + 
  theme(text = element_text(size=8), 
        axis.title.x=element_blank(),
        legend.title = element_blank(),
        legend.position = "right",
        legend.box = "vertical",
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(4.5, "mm"))
p
```

### The Rise and Rise of the World Wide Web
According to the International Telecommunication Unions (**ITU**) 2015 figures, Internet penetration has grown from just over 400 millions users (6 per cent of global population) in 2000 to 3.2 billion users in 2015 (43 per cent of global population). This includes around 2 billion users from developing countries [@duttaGlobalInformationTechnology2015]. ICTs bring a broad range of benefits and are recognised as a key to eradicating poverty and unemployment. They enable and facilitate the building of a people-centred, inclusive and development-oriented Information Society, where everyone can create, access, utilise and share information and knowledge. This enables individuals, communities and peoples to achieve their full potential in promoting their sustainable development and improving their quality of life [@InformationCommunicationTechnologies]. 
In addition to a rapidly growing internet user-base both in the developing and developed world, the nature of internet usage has fundamentally changed. Once the purview of academics, engineers and computer scientists sending tiny packets of information back and forth, there are now some 2.5 quintillion bytes of data created each day by all manner of users, industries and sensors, to name a few [@DataNeverSleeps].

```{r internetAccessPreprocessing, message=FALSE, warning=FALSE, include=FALSE}
internet <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/datasets/number-of-internet-users-by-country.csv')
colnames(internet)[4] <- 'Number'
internet$Continent <- countrycode(sourcevar = internet$Code,
                            origin = "genc3c",
                            destination = "continent")
internet <- na.omit(internet)
internet <- internet %>% group_by(Continent, Year) %>% summarise(Number = sum(Number))
internet <- internet %>% filter(Year != 2017)
```

```{r internetAccessPlot, echo=FALSE, fig.align='center', fig.cap="Internet access by total number of population, Source: The World Bank, World Development Indicators", fig.height=2.75, fig.width=4, message=FALSE, warning=FALSE}
www <- ggplot(internet, aes(x = Year, y = Number, fill = Continent))
www <- www + geom_area(alpha=0.9 , size=.2, colour="white") + 
  scale_fill_brewer(palette = 'Set1') +
  theme_grey() +
  scale_x_continuous(limits = c(1990, 2015), expand = c(0.02, 0.02)) + 
  scale_y_continuous(limits = c(0, 4000000000)) + 
  #scale_y_continuous(labels = whole)
  labs(title="Internet access by total number of population",
       x="Year",
       y="Number of people") + 
  theme(text = element_text(size=8), 
        axis.title.x=element_blank(),
        legend.title = element_blank(),
        legend.position = c(0.125,0.725),
        legend.box = "vertical",
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(5, "mm"))
www
```

### Forging a New Technological Paradigm
Such rapid and widespread internet adoption has created a seemingly insatiable demand for exponentially greater computational power and digital storage capacity. This has led to a new and ubiquitous technological paradigm: Cloud Computing. Cloud Computing is succinctly defined as; the practice of using a network of remote servers hosted on the Internet to store, manage, and process data, rather than a local server or a personal computer [@CloudComputingDefinition]. 

As with the initial rise and widespread implementation of the Internet, Cloud Computing itself acts as a facilitator for new technologies. One such example being the Internet of Things (**IoT**) paradigm. The IoT can be surmised as the extension of the Internet and the Web into the physical realm, by means of the widespread deployment of spatially distributed devices with embedded identification, sensing and/or actuation capabilities, allowing objects to be sensed or controlled remotely through the internet [@danielemiorandiInternetThingsVision2012], [@binghuangServiceOrientedComputing16th2018], [@luigiatzoriInternetThingsSurvey2010]. IoT thus represents a convergence of real-world objects and digital objects into a unified cyber-physical system. 

### The Bigger Picture 
Considering the bigger picture of societal benefit, as seen in figure \ref{fig:globalPovertyPlot}, from 1990 through 2015 the number of people living in extreme poverty has dropped by more than half. As evidenced by Figure \ref{fig:internetAccessPlot}, over the same time period, the percentage of people with access to the internet has move from around 1 per cent to an average of around 50 per cent (this trend is now moving exponentially as a function of time). And, figure \ref{fig:populationGrowthPlot} shows that over the last 100 years, the human population has experienced unprecedented growth from 1 billion individuals to in excess of 7 billion. Mankind have thus simultaneously increased our population, increased of technological development and decreased poverty (to name but a few, ‘key performance indicators’).

```{r populationGrowthPreprocessing, message=FALSE, warning=FALSE, include=FALSE}
population <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/datasets/world-population-by-world-regions-post-1820.csv')
population$Continent <- countrycode(sourcevar = population$Code,
                                    origin = "genc3c",
                                    destination = "continent")
population$Population <- population$Population / 10000
population <- na.omit(population)
population <- population %>% group_by(Continent, Year) %>% summarise(Population = sum(Population))
population$Population <- population$Population * 10000
#population <- population %>% summarise(Population = sum(Population))
#internet <- internet %>% filter(Year != 2017)
```

```{r populationGrowthPlot, echo=FALSE, fig.align='center', fig.cap="Human population by continent, Source: The World Bank, World Development Indicators", fig.height=2.75, fig.width=5, message=FALSE, warning=FALSE}
popu <- ggplot(population, aes(x = Year, y = Population, fill = Continent))
popu <- popu + geom_area(alpha=0.9 , size=.2, colour="white") + 
  scale_fill_brewer(palette = 'Set1') +
  theme_grey() +
  scale_x_continuous(limits = c(1885, 2015), expand = c(0.02, 0.02)) +
  scale_y_continuous(limits = c(0, 8000000000)) + 
  labs(title="Human population by continent",
       x="Year",
       y="Number of people") + 
  theme(text = element_text(size=8), 
        axis.title.x=element_blank(),
        legend.title = element_blank(),
        legend.position = c(0.11,0.75),
        legend.box = "vertical",
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(5, "mm"))
popu
```

The success of modern human society is not without consequence. All of the benefits our society has enjoyed from the development, production and deployment of technology, has required vast amounts of energy. This energy has, since the industrial revolution, primarily been derived from the burning of fossil fuels, as illustrated by Figure \ref{fig:globalEnergyPlot}. 

```{r globalEnergyPreprocessing, message=FALSE, warning=FALSE, include=FALSE}
energy <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/datasets/primary-energy-consumption-by-source-and-region.csv')
energyCols <- c('Entity', 'Code', 'Year', 'Oil(TWh)', 'Gas(TWh)', 'Coal(TWh)', 'SolarPV(TWh)',
                'OtherRenewables(TWh)', 'Wind(TWh)', 'Nuclear(TWh)', 'Hydropower(TWh)')
colnames(energy) <- energyCols
energy$Continent <- countrycode(sourcevar = energy$Code,
                            origin = "genc3c",
                            destination = "continent")
energy <- na.omit(energy)
energy <- gather(energy, `Oil(TWh)`, `Gas(TWh)`, `Coal(TWh)`, `SolarPV(TWh)`,
               `OtherRenewables(TWh)`, `Wind(TWh)`, `Nuclear(TWh)`, `Hydropower(TWh)`,
               key = "Source", value = "Consumed")
energy <- energy %>% group_by(Continent, Year, Source) %>% summarise(`Consumed` = sum(`Consumed`))
energy <- energy %>% group_by(Year, Source) %>% summarise(`Consumed` = sum(`Consumed`))
```

```{r globalEnergyPlot, echo=FALSE, fig.align='center', fig.cap="ADD TEXT, Source: Our World in Data", fig.height=2.75, fig.width=5, message=FALSE, warning=FALSE}
p <- ggplot(energy, aes(x = Year, y = Consumed, fill = Source))
p <- p + geom_area(alpha=0.9 , size=.2, colour="white") + 
  scale_fill_jco() +
  theme_grey() +
  scale_x_continuous(limits = c(1965, 2015), breaks = c(1965, 1970, 1975, 1980, 1985, 1990, 
                                1995, 2000, 2005, 2010, 2015), expand = c(0.02, 0.02)) +
  labs(title="Primary energy comsumption by source, World",
       x="Year",
       y="TWh") + 
  theme(text = element_text(size=8), 
        axis.title.x=element_blank(),
        legend.title = element_blank(),
        legend.position = c(0.155,0.70),
        legend.box = "vertical",
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(4.25, "mm"))
p
```

The International Panel on Climate Change (IPCC) finds that Human activities are estimated to have caused approximately 1.0°C of global warming above pre-industrial levels, with a likely range of 0.8°C to 1.2°C. Global warming is likely to reach 1.5°C between 2030 and 2052 if it continues to increase at the current rate [@intergovernmentalpanelonclimatechangeGlobalWarming2018]. 

Climate change poses an existential threat to modern human civilisation, with warming of between 1.5°C and 2°C predicted to cause increases in mean temperature in most land and ocean regions, hot extremes in most inhabited regions and heavy precipitation changes in some regions. Additionally, increases in ocean temperature as well as associated increases in ocean acidity and decreases in ocean oxygen levels are projected to reduce risks to marine biodiversity, fisheries, and ecosystems, and their functions and services to humans. Taken together, these effects will lead to risks of the health, livelihoods, food security, water supply, human security, and economic growth of mankind [@intergovernmentalpanelonclimatechangeGlobalWarming2018].


```{r warmingTrend, echo=FALSE, fig.cap="Atmospheric Changes with respect to Carbon Emissions and Global Warming - Observer and Projected. Source: Intergovernmental Panel on Climate Change, 2018", fig.align='center', out.width = '80%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/warmingTrend.png')
```
It is therefore imperative moving forward as a species that all steps are be taken to mitigate the emission of greenhouse gases and abate the advance of anthropomorphic climate change. The scale of the challenge is such that technology itself will prove critical in effectively combating this existential threat to civilisation. When considering energy consumption, the industrial sector (including the non-combusted use of fuels) currently consumes around half of all global energy and feedstock fuels, with residential and commercial buildings (29%) and transport (21%) accounting for the remainder [@EnergyDemandSector]

```{r SectorEnergyPlotProcessing, message=FALSE, warning=FALSE, include=FALSE}
energyBySector <- read_table2("/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/EnergySpecs/ElectricityTFCbySector.txt")
energyBySector <- gather(energyBySector, `Industry`, `Residential`, `CommercialAndPublicServices`, 
               `Transport`, `Other`, key = "Source", value = "MTOE")
```

```{r sectorEnergyPlot, echo=FALSE, fig.align='center', fig.cap="ADD TEXT", fig.height=2.75, fig.width=5, message=FALSE, warning=FALSE}
p <- ggplot(energyBySector, aes(x = Year, y = MTOE, fill = Source))
p <- p + geom_area(alpha=0.9 , size=.2, colour="white") + 
  scale_fill_npg() +
  theme_grey() +
  scale_x_continuous(limits = c(1975, 2015), breaks = c(1975, 1980, 1985, 1990, 
                                1995, 2000, 2005, 2010, 2015), expand = c(0.02, 0.02)) +
  labs(title="Electricity total final comsumption (TFC) by sector, World",
       x="Year",
       y="TWh") + 
  theme(text = element_text(size=8), 
        axis.title.x=element_blank(),
        legend.title = element_blank(),
        legend.position = c(0.185,0.77),
        legend.box = "vertical",
        legend.background = element_rect(fill=alpha('white', 0)),
        legend.key.size = unit(4.5, "mm"))
p
```

### Optimising Global Energy Usage

Thus far, our staggering global achievements, including lifting hundreds of millions out of poverty, rapid global technological deployment and strong population growth has been inexorably linked to increased energy consumption. This in turn has led to perturbations in atmospheric chemistry, in the form of anthropogenic climate change (amongst many other environmental challenges), which fundamentally threatens our global achievements. It therefore stands that the key to continued human prosperity is to de-couple growth in energy demand from economic growth. In this work we will explore the possibility of reducing energy consumption via the optimization of services to end users in an IoT Smart Home.

### The IoT Smart Home & Service Oriented Computing

As mentioned above, residential and commercial buildings account for 29% of energy demand globally [@Consumption]. In this work we propose that an avenue for reduced energy consumption is the optimization of existing services and utilities to end users in an IoT Smart Home. In this proposed smart home, the daily activities of the end user can be performed with the support of a personalised artificial intelligence (AI) system, such that the timing and manner of energy intensive activities is optimised to reduce overall energy consumption, whilst still considering the level of convenience afforded to the end user. This work uses sensor data analogous to what would be expected to be produced from a sensor rich IoT smart home and considers the interplay between end user convenience; predictive analytics; predictive service offering; energy consumption; demand response and smart electricity grids.

## Aim

### Research Questions

The aim of this work is to use sensor data from X to consider Y in the context of Z.

### Research Contributions

The aim of this work is to use

```{r section1DUMP, message=FALSE, warning=FALSE, include=FALSE}
rm(energy)
rm(energyBySector)
rm(internet)
rm(p)
rm(population)
rm(poverty)
```

\pagebreak

# Preliminaries

Significant growth in digital interconnectivity over the last 20 years has given the internet a pivotal role as an essential element of economic growth [@haseebDoesInformationCommunication2019]. Cloud computing and the IoT paradigm has enabled the association of previously disparate fields into a larger coherent framework. Namely, smart home appliances, demand-response, energy consumption, predictive analytics predictive service offering and end user convenience.  This work proposes a framework for the association of sensor data into a model which can do X Y Z. **Mention internet in general?**

## Related Work

### Ubiquity of the World Wide Web and Implications for Energy Consumption

The International Telecommunication Union estimated about 3.2 billion people, or almost half of the world's population, would be online by the end of the 2015  [@InformationCommunicationTechnologies]. The impact of internet usage and mobile cellular subscriptions (**ICTs**), globalization, electricity consumption, financial development, and economic growth on environmental quality has been examined [@haseebDoesInformationCommunication2019]. By using 1994–2014 panel data of the Brazilian, Russian, Indian, Chinese & South African (**BRICS**) economies, empirical results demonstrate that rise in both internet usage and mobile cellular subscription ICTs likely mitigates CO2 emissions.

**BRIDGE** – Greater internet availability / connectivity facilitates the deployment of smart electricity grids  

### Smart Metering

A Smart meter is an electronic device that records consumption of electric energy and communicates the information to the electricity supplier for monitoring and billing. In the Unites States (for example) smart meters are a significant part of the larger Smart Grid infrastructure, and as far back as 2012, had been installed in over 25 million U.S. homes [@hornePrivacyTechnologyNorms2015]. Smart Meters transmit information about consumer electricity use to utility companies at vastly shorter time intervals than before and this information helps utility companies to coordinate power supply and demand, detect outages, implement time-of-use and dynamic pricing, and in other ways improve system efficiency and reliability. Additionally, these data are also becoming increasingly available, and sought-after, by end-users themselves. Indeed, the main purpose of provisioning smart metering data to end users is to encourage the use of less electricity, by better informing users of their consumption patterns [@jui-shengchouCloudForecastingSystem2019]. In the aggregate, these savings can significantly reduce national energy use and curb energy emissions while addressing pressing geopolitical and environmental concerns related to energy security and sustainability [@graabSmartGridSmart2011]. 

Since the widespread deployment of smart meter technology, there has been huge interest in the capability of these technologies with respect to the technical capacity of utility companies to manage demand (through demand response programs), incorporate renewable sources of electricity into the system, and increase the overall efficiency and reliability of the system [@hornePrivacyTechnologyNorms2015]. **It is worth noting, however, that the scenario described above relies on end user intervention.**

### Electricity Demand Response, Smart Electricity Grids and Smart Home Data

Predicting and influencing residential energy use has been the subject to extensive study (**REF**). Literature indicates that factors such as occupant behaviour and socio-economic status are important **[15 from Cetin]**. Nielsen attributed 36% of variation in energy consumption of homes to lifestyle and occupant behaviour, and 64% to socio-economic influences. This is exemplified by the work of De et al, who show that in developing nations, cooking consumes up to 90% of the overall residential energy consumption and is mainly based on non-renewable energy (International Energy Agency, 2006; De et al., 2013). Other factors such as climate zone, number of occupants, income level, age of home, and size of home have also been correlated with home energy use (K.S. Cetin, 2014). The has been much work both on using smart meter data to evaluate consumer behaviour, and the interplay between smart appliances and shifting households’ electricity demand. 

Kavousian et al identify the need for developing an analytical method that can leverage 15-minute or 30-minute interval energy consumption data produced via smart metering in order to improve the effectiveness of energy efficiency programs. They note that utilities spend millions of dollars annually to improve appliance energy efficiency. By way of example, in 2013 utilities in California US spent $80 M USD on appliance and plug load efficiency programs, the highest expenditure among all utility energy efficiency programs (**CPUC, 2013 FIND REF**). A need for analytical methodologies that can process smart meter data and allow energy reduction savings to be identified is demonstrated. Using a smart meter dataset of 4231 households, containing information on electricity consumption at 30-minute intervals, with aggregated local weather data, the authors use smart meter data to rank residential appliance efficiency. Various control methodologies are embedded into the analytical process, taking into account building type, building size (e.g., apartment, detached), household type (e.g., de facto, single, dependents), respondent age and heater type (e.g., electric or gas). The data set included 120+ household variables, many of which were highly correlated, model selection techniques were used to successfully reduce dimensionality. A set of load profiles were compiled based on the results (where normalized load was plotted as a function of hour of day). It was determined that household behaviour and demographic can be used to generate positive and negative energy efficiency coefficients with respect to load profiles. This work has implications for energy grid planning – for example in high-density housing versus suburban housing. 

Karjalainen considers the manner in which smart meter data is communicated to end users, with the principal purpose of the paper being to study what kind of electricity consumption feedback consumers understand and prefer (Karjalainen, 2011). It is noted that the most effective feedback tools for engaging households in reducing energy consumption are both computerized and interactive. In this work a series of options for feedback is gathered, for example consumption (kHh), power (W), cost ($), environmental impact (e.g., kg CO2), total for household, disaggregation by appliance, real time, hour, day, and so on. Examples of feedback options are shown in Figure **X**, below.

```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/dash1234.png')
```

```{r echo=FALSE, fig.cap="Option Five, Six, Seven and Eight of the smart meter data dashboards proposed by Karjalainen", fig.align='center', out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/dash5678.png')
```

The results of qualitative participant interviews clearly showed that while some consumers are very interested in saving household electricity, other consumers show only a little interest (**NOTE they were asked about saving electricity, not about saving money or convenience**). When specifically asked to list what measures (if any) were typically taken at home to realize reductions in energy usage, some respondents listed numerous measures, while others just said they turn lights off in rooms that are empty. The author also found some participants were unaware of differences in stand-by versus active modes of operation for electrical appliances, resulting in practising inefficient energy saving measures in the home environment. When trialling feedback prototypes to participants, two main issues were encountered: (1) many people are not familiar with scientific units and do not understand the difference between W and kWh and (2) many people do not understand how carbon dioxide emissions are related to electricity consumption. It is perhaps surprising that the overall most popular prototype was ‘6’, below. Perhaps because unlike the other prototypes, this clearly (the most clearly) articulates cost. The concept of convenience was absent entirely from consideration.

In the domain of supply and demand economics, consumer behaviour and smart appliances Kobus et al investigated if households can shift their electricity demand to times when electricity is abundantly available [@charlotteb.a.kobusReallifeAssessmentEffect2015]. Using a household electrical monitoring system (**EMS**) coupled to a smart appliance (smart washing machine), photovoltaic cells and the electricity grid, they were able to show that households can shift 10–77% of the electricity demand of their washing machine. This longitudinal study was conducted via the participation of 50 Dutch households over a period of one year. By utilizing an EMS which shows appliance status, dynamic tariff information and current status of household electricity usage, participants are able to schedule smart washing machine use in such a way that cost is minimized.

```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '60%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/kobusDash.png')
```
Their work is in response to two major challenges to domestic electricity supply and demand. **REMOVED: It is well observed that over electricity grids, demand shows a pattern over time, influenced by natural and social circumstances. This poses a great challenge when considering renewable energy sources like PV – people are generally not at home when local electricity is abundantly produced (for example the daylight hours of Monday till Friday)** The first being that the amount of distributed renewable electricity generation is increasing with time (for example, as more households install photovoltaic (**PV**) panels), and the second that electricity demand will continue to significantly increase moving into the foreseeable future. These developments pose great challenges to ‘traditional’ power systems, where supply follows demand entirely. Smart grids are proposed as a potential solution to facilitate the affordable introduction of cleaner electricity producing and consuming technologies. 

Cetin et al consider electricity usage of appliance, as when aggregated, this accounts for approximately 30 per cent of electricity used in the residential building sector [@k.s.cetinApplianceDailyEnergy2014]. This, together with small appliances, home electronics and lighting, account for more than 2/3 of total residential electricity use. Cetin et al also highlight that influencing ‘time of use’ is becoming increasingly important to control the stress on today’s electrical grid infrastructure. The authors seek to determine when refrigerators, clothes washers, clothes dryers and dishwashers are predominantly used (and thus consume energy) and what causes variation in their use. Using disaggregated energy data from 40 homes over a period of 1-year, normalized load profiles for the four target appliances were generated, as a percentage of daily electricity load.

```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '65%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/cetinResults.png')
```
It was found that the refrigerator had the most consistent consumption profile across all homes surveyed. Influencing factors for the refrigerator were correlated to both indoor and outdoor temperature, however effect was found to be minimal. The clothes washer and dryer were found to have the greatest variation in normalised energy use by hour, with the greatest period of use from 9am until 2pm. The dishwasher had distinct peaks in load profile at 9am and 10pm. The authors found that user-dependent appliance use patterns vary more between homes and between days than automated appliances, weekday and weekend use patterns of appliances are similar, and that electricity use varies more between houses during peak use times of day than during low-use times.
Murray et al further explore the topic of home appliance energy consumption using smart meter data, specifically pertaining to residential activities around food storage and preparation (D.M. Murray, 2018), with the aim of providing a model that can easily be applied to existing smart meter energy datasets. The authors use real-time energy consumption data from microwaves and ovens, from which user behaviour / desired outcome can be implicitly associated (for example, oven usage on a weekday between the hours of 6pm and 8pm can be associated with the behaviour of preparing dinner). Consideration is given to the concept of the RisingEdge (when an appliance is switched on or moves to a high consuming state) and FallingEdge (when an appliance is switched off or moves to a low consuming state) of power consumption. Accurate energy consumption models for major cooking appliances are successfully constructed, further re-enforcing the value and widespread applicability of residential smart meter data. 



In this work, 

This supports __ that smart meters represent a valuable source of data that can be used for a variety of purposes, beyond just real-time monitoring. For example the model constructed by Murray et al does not rely on difficult to obtain parameters such as food type, temperature and weight.
it has been shown that …

*	Authors are able to use smart meter data to build accurate energy consumption models of major cooking appliances
*	Smart =
*	Using consumption data from real homes, the authors are able to create scalable mathematical models to account for energy consumption associated with food preparation
*	Indeed, from an energy-efficiency perspective, targeted energy feedback can be provided on consumers appliance usage habits and activities (Stankovic et al., 2016) leading to informed energy savings programmes and retrofit strategies for replacing appliances.
* And if this variation can be influenced??

_Demand Response allows for the management of demand side resources in real-time; i.e. shifting electricity demand according to fluctuating supply. When integrated into electricity markets, Demand Response can be used for load shifting and as a replacement for both control reserve and balancing energy. These three usage scenarios are compared based on historic German data from 2011 to determine that load shifting provides the highest benefit: its annual financial savings accumulate to €3.110M for both households and the service sector. This equals to relative savings of 2.83% compared to a scenario without load shifting._

[@article{feuerriegel2016}] discuss electricity demand response - allowing the management of demand side resources in real-time.
**Based on Darby and McKenna (REF), we define demand response and a household action (automated, manual, or both) due to which electricity use is shifted in time in response to a price signal of other stimuli. This can result in more efficient usage of the available sustainable electricity, like self-consumption of on-site PV electricity (REF) and peak demand reductions (REF) - all from @article{kobus2015}**





### Predictive Modelling and Forecasting

In the domestic predictive energy consumption space Basu et al consider home automation systems linked via a communication network to enable interaction, data collation and control of appliances remotely by end users [@kaustavbasuApplianceUsagePrediction2012]. The potential competing priorities of energy savings versus comfort optimization for home occupants is discussed. The objective of this work is to propose a learning system that is able to help the home automation system compute an energetic plan that is also satisfactory to user requests. Taking into account correlation between appliances, a time-series based multi-label classifier is used to predict appliance usage up to one hour into the future.

The attribute construction technique of Knowledge Extraction applied. This process aims to extract novel attributes from underlying substructures in the training instances in the form of sub-events, for example periodicity in data. This Knowledge Extraction process is similar to the implicit associations used by Murray et al, in the analysis of energy consumption for food preparation [@d.m.murrayApplianceElectricalConsumption2018]. The substructures are then fed to a propositional learner. The proposed model is trained in an iterative manner and attempts to take into account all the possible information based on consumption data, time of the event and meteorological information. Time is specifically model time as a periodic variable, segmenting on hour of day and day of week, noting that this takes into account the periodic nature of human behaviour. Using BR1, LP, CC1, CC2 and MLk machine learning algorithms, the precision, recall and accuracy for a variety of electricity-consuming appliances is determined (where each appliance constitutes a target variable over a set of iterations). In the evaluation phase they find that user behaviour toward an appliance is highly variable and the predictability of an appliance is dependent on the regularity of usage patterns of the inhabitants. It is specifically noted that _it is therefore very difficult for now to propose a generic methodology of appliance prediction for private houses_.

```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '80%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/basuTree.png')
```
Chou et al consider global energy consumption in the residential housing sector in the context of domestic energy information system (EIS) and smart meter system (smart grids) technologies [@jui-shengchouCloudForecastingSystem2019]. They note two influential factors on overall residential energy consumption, the first being the type and number of electrical appliances and the second being the usage of these appliances by occupants. It is proposed that a major challenge for people who are willing to save energy at home is a lack of information about their energy consumption. To test this hypothesis, the authors develop a web-based energy information management system for the power consumption of home appliances that monitors the energy load of a home, analyses its energy consumption based on machine learning, and then sends information to various stakeholders. Interaction with end-users in the home is achieved via energy dashboards and emails. The authors propose that end-users of this system can use forecast information and anomalous data to enhance the efficiency of energy usage in their buildings especially during peak times by adjusting the operating schedule of their appliances and electrical equipment. **In order to achieve the desired model (???)**

During this study, an EIS with 5 main components was installed in an experimental building. The parts were as follows; (1) the internal communication network, (2) the data management infrastructure, (3) the automated prediction system, (4) the web-based system and dashboard and (5) the early warning system. Data from both the smart meter and from sensors was used to compile the model, including timestamps (YYYY-MM-DD hh:mm:ss), outdoor temperature (ºC), total building energy consumption (kWh),  aggregated energy consumption data (e.g., second floor lighting) and individual appliances. The backend of the application was notably cloud-hosted.

*	Improve consumer satisfaction by providing real-time services that enable end-users to monitor the energy consumption easily.
*	Energy Information System - personalised terminal in domestic home which provides real-time or daily updating of hourly energy consumption data allows users to address building performance issues that are otherwise difficult to identify.
*	The data stream from the smart meters arrives at 15 min intervals, generating 96 data points daily for each smart meter.
*	The hardware of the DBMS consists of the communication system, the smart meter system, the environmental sensor system, and the server system, and the software programs that are used to process the data written in MATLAB. A DBMS stores real-time data that are retrieved from the smart grid metering infrastructure, including real-time energy consumption data, information about appliances, data from temperature and humidity sensors, and analytical results, including predicted electricity consumption, and data on electricity-saving alternatives. Moreover, the DBMS stores electrical parameters such as voltage, current, power, frequency, and power factor.
*	The proposed prediction model (i.e.,SARIMA-MetaFA-LSSVR) consists of stage 1, involving univariate linear technique, and stage 2, involving optimized multivariate nonlineartechnique tofit the energy consumption data. SARIMA is adopted to handle the linear component, whereas Meta FA-LSSVR is used to model the nonlinear part of energy consumption data. In particular, the MetaFA is employed for automatically tuning LSSVR hyper parameters, thus improving the overall prediction performance.
*	Output
*	A web-based system to display energy consumption that is measured in real time with previously predicted values.
*	Early-warning system integrated into web-based system to detect anomalous power consumption and provide warnings (notifications) thereof to users.
*	The real-time prediction model in web-based system is designed and implemented based on our previous work[20,33,49], which developed a novel sliding window metaheuristic optimization-based machine learning system to analyze time-series data that are generated by a smart grid to efficiently forecast energy consumption one day in advance.

### Smart Home and Energy Efficiency

[@article{bober2009distributed}NOT IN LIB] - As recently as 2009 it was posited that "The proposed model allows for introduction of power  priorities for consumer’s electrical equipment by the importance of their functions. The relevance of the functions carried out by the data device is evaluated by each consumer individually. The relevance of functions and priorities assigned to power mode / groups of electrical equipment can be changed over time."  
'Haseeb et al [@article{haseeb2019does}NOT IN LIB] have (through review) shown that at a macro scale, global adoption of internet reduces energy consumption' + At the micro scale (that is, the behaviour of individuals...)

### IoT and End User Convenience

The term Internet-of-Things (**IoT**) is used as an umbrella keyword for covering various aspects related to the extension of the Internet and the Web into the physical realm, by means of the widespread deployment of spatially distributed devices with embedded identification, sensing and/or actuation capabilities [@danielemiorandiInternetThingsVision2012]. The IoT paradigm is fundamental to this work, representing the confluence of multiple technological advancements [@luigiatzoriInternetThingsSurvey2010] including, ubiquity of internet access, the availability of high-performance internet connectivity, inexpensive consumer electronics with embedded sensing and control systems, automation, real-time analytics, machine learning, commodity sensors and embedded systems. In the IoT paradigm, digital and physical entities can be linked, by means of appropriate information and communication technologies, to enable a whole new class of applications and services [@danielemiorandiInternetThingsVision2012]. 

One consequence resulting from the widespread deployment of consumer electronics with IoT capability is the evolution of the Internet from interconnecting computers to interconnecting things. Figure X, below, shows XYZ. 

```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '65%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/oldNewParadigm.png')
```

### Heading???

Huang et al propose a novel service mining framework to personalize services in an IoT-based smart home (Bing Huang, 2018). This work considers the notion of personal 'convenience' as a driving force behind the provisioning of services to end users. That is, in an IoT smart home, where everything is interconnect, can services (for example, the switching on or off of a light) be automatically served to users such that their level of effort (to interact with their surroundings) will be diminished, and thus their level of convenience will be increased?


```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '50%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/huangA.png')
```

Using this framework, IoT services can be composed in a periodic fashion, for the convenience of the end user / consumer.

* Use intelligence to compose IoT services for the convenience of the end user / consumer

The input for this work was data from domestic IoT services, with a corresponding timestamp. The end user in this scenario performs daily activities by interacting with IoT services, these interactions are recorded as IoT service event sequences. A novel PCMiner algorithm (Periodic Composite IoT Service Miner) 

Periodic composite IoT services can be loosely defined as the composite IoT services’ repeating occurrence at certain locations with regular time intervals

The output for this work was an IoT service model and a composite IoT service model (based on spatio-temporal features). A periodic composite IoT service model to represent the regularity of composite IoT services occurring at a certain location at a certain time interval


```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '50%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/huangC.png')
```

Critical to note that in this work model composition is reliant on temporal, proximity and duration features.

Output: Spatio-temporal proximity model 

* Filter out insignificant spatio-temporal IoT service relationships
Categories (e.g., objective):
* Quantify the correlation strength of among IoT services from time and location aspects
* IoT devices are highly heterogeneous in terms of supporting infrastructure ranging from networking to programming abstraction
* Hiding (abstracting) complex and diverse supporting infrastructure
* Proximity and correlation strength

Service Mining on the Web @article{zheng2009}

In the domain of Service Mining and Service Recognition, Zheng et al aim to provide end users with value-added services through the composition of existing services (output = composed web services). They attempt to craft a Service Mining Tool 
*	More generalised
* Technology = Service mining tool, web service mining
*	Output = composed web services
*	Aims at providing value-added services through composing existing services
*	It is essential to be able to proactively discover opportunities for composing useful services, even when the goals are unspecified at the moment, or simply hard to imagine or unknown
*	Much like the easy access to a glut of data has provided a fertile ground for data mining research, there is an expectation that an increase in Web services’ availability will also spur both the need and opportunities to break new ground on Web Services Mining
*	We define Web Service Mining as a bottom-up search process aimed at the proactive discovery of potentially interesting and useful web services from existing services.

**Two Main Challenges**
*	Combinatorial Explosion
*	Evaluation of interestingness and usefulness

### Summary

In summary there is lots of variability and this informs the construction of our model – it must be robust. By stripping the time variable this greatly simplifies modelling. As mentioned in the work of Murray et al (Appliance electrical consumption modelling at scale using smart meter data) smart home data contains implicit associations, the example given being that, if a person is using the oven between the hours of 6pm and 8pm on a weekday, it can be assumed they are preparing dinner – there is no need to explicitly determine this. In a similar way, we **believe** that we can implicitly obtain spatial and temporal information about how a series of appliances (electrical and non-electrical) are interrelated.

As demonstrated above, there has been a vast swathe of work done in the areas of smart meter data and predictive analytics, X, Y & Z. What is, however, of significant note, is the absence of holistic consideration with respect to energy consumption, smart home data, predictive analytics, IoT and end user convenience. As demonstrated above, numerous studies have found that creating and maintaining behaviour (such as monitoring a smart meter and then intervening) is … shit. 

Due to n = 1, it would be a fair criticism to say that the methodology employed cannot claim to be robust… this is fair. However, the 
Kavousian et al thoroughly analyse the effect of household variables on overall energy efficiency. Our analysis (n=1) has no such … 
Take for example Consumer preferences for feedback on household electricity consumption (REF). The work considers, in isolation, the concept of saving electricity. For example during participant interviews, the line of questioning is framed around electricity saving NOT cost or convenience.   

1. The cost of the sensors is negligible (EXCEPT for discussion later RE top 5 features)
2. The energy requirement for the sensors is negligible
3. In our proposed model, computation is performed in the cloud
4. An app is available to interact with the smart home
5. All of the electrical appliances in our smart home can be remote controlled to some extent

* What about, instead of responding to the average time, the antagonistic AI responded to price signals in the market? 


## The Data Set

The datasets were created during the thesis _Activity Recognition with End-User Sensor Installation in the Home_ by Randy Joseph Rockinson, Submitted to the Program of Media Arts and Sciences, School of Architecture and Planning, in partial fulfilment of the requirement for the degree of Master of Science in Media Arts and Sciences at the Massachusetts Institute of Technology (MIT) February 2008 (REF). 

The aim of Rockinson was to considered the effect of end user versus professional installation of a sensor array in the home - on the basis that, if installation of sensors is to be considered as a high initial cost, and a barrier to entry for end users wanting this technology, is there a difference if a professional versus an end user performs the installation? 

Between 80-100 reed switch sensors where installed in two single-person apartments collecting data about human activity for two weeks. The sensors were installed in everyday objects such as drawers, refrigerators, containers, etc to record opening-closing events (activation deactivation events) as the subject carried out everyday activities.

```{r echo=FALSE, fig.cap="ADD TEXT", out.width = '100%'}
#knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/')
```


\pagebreak

# Data Preprocessing & Visualisation

## Importing & Preprocessing the Activities Meta Data
The dataset `S1Activities.csv` was imported into the interactive development environment. These data contains a tabulated summary of Heading, Category, Subcategory and a corresponding unique code. After importation, the dataset has dimensionality of [3, 33], with `Heading`, `Category` & `Subcategory` present as non-null objects, as seen in Table \ref{tab:TAB_S1ActivitiesData} below. The attribute `Code` (which codefies the unique set of Heading, Category & Subcategory) was imported as an index value (n=33). At this time, the activities data will be kept in it's native state and will not be subject to preprocessing.
```{python READ_activities, include=FALSE}
ds = pd.read_csv(PATH + '/datasets/S1Activities.csv', index_col = 'Code') 
```

```{r TAB_S1ActivitiesData, echo=FALSE, results='asis'}
head(py$ds, n=5) %>% kable(format = "latex", caption = "The S1 activities dataset", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

## Importing & Preprocessing the Sensor Meta Data
The dataset `S1sensors.csv` was imported into the interactive development environment. These data contains a tabulated values for Sensor ID, Room and Sensor Descriptor (e.g., light switch), with no header row present in the original dataset. After importation, the dataset has dimensionality of [3, 76], with header 0, 1 & 2 corresponding to SensorID, Room & Sensor Descriptor, respectively, as seen in Table \ref{tab:TAB_sensorData}. As all attributes are nominal, they were imported as String data types. 
```{python READ_sensors, include=FALSE}
dsS1Sensors = pd.read_csv(PATH + '/datasets/S1sensors.csv', 
                          dtype={0:str, 1:str, 2:str},
                          index_col = None, header = None)
```
```{r TAB_sensorData, echo=FALSE, results='asis'}
head(py$dsS1Sensors, n=5) %>% kable(format = "latex", caption = "The S1 sensor meta data", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```
After importation these data were checked for duplicate values. The `Sensor ID` attribute had a 76 unique values, the `Room` attribute had only 7 unique values and the `Activity` attribute had 28 unique values. Examples of the degeneracy in these attributes can be seen in Table \ref{tab:TAB_sensorData}, e.g., Bathroom & Light Switch. Room[1] and Activity[2] were stripped of whitespace, coerced to lowercase and concatenated using an underscore. The concatenated vector was then bound to the dataframe and the column names updated, resulting in \ref{tab:TAB_sensorDataProcessed}, below.
```{python FUNC_S1sensorsPreprocessing, include=FALSE}
# %run -i S1sensorsPreprocessing.py
temp1 = dsS1Sensors[1] + '_' + dsS1Sensors[2]   # Creating a new vector of concatenated column 1 & 2
temp2 = temp1.str.replace(" ", "")              # Removing whitespace between strings
temp3 = temp2.str.lower()                       # Changing all text to lowercase
temp4 = temp3.str.strip()                       # Striping any remaining whitespace
dsS1Sensors[3] = temp4                          # Adding the precessed vector back into the dataset
dsS1Sensors.rename(columns={0:'subActNum', 
                            1:'room',
                            2:'activity',
                            3:'concat',
                            4:'energyReq'}, 
                   inplace=True)
```
```{r TAB_sensorDataProcessed, echo=FALSE, results='asis'}
head(py$dsS1Sensors) %>% kable(format = "latex", caption = "The first iteration of processed S1 sensor meta data", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```
The number of unique values in `dsS1Sensors.subActNum` is checked once more and found to be n=76, indicating no degeneracy in this attribute. The number of unique values in `dsS1Sensors.concat` is found to be n=41, indicating the presence of degeneracy. This was investigated by aggregating the duplicate attributes, which are summarised in Table \ref{tab:TAB_numDupes}, below.
```{r TAB_numDupes, echo=FALSE, results='asis'}
dupes <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/datasets/dupes.csv')
head(dupes, n=10) %>% kable(format = "latex", caption = "Summary of the number of degenerate values (n) for each subactivity, where applicable.", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```
Referering to the work of Rockinson (from where the data originated) it is was determined that these duplicate values are theresult of multiple sensors with extremely similar functionality [@rockinsonActivityRecognitionHome]. For example, kitchen_burner has a value of n=4, accounted for by the presense of one sensor per burner in the original work. While this level of granularity may provide an avenue for further analysis, for the purposes of this research such values serve to significantly increase the dimensionality of the overall dataset, with a low corresponding gain in information. High dimensionality can also lead to difficulties in machine learning models and challenges with data visualisation. The sensor class kitchen_cabinet has a value of n=15, indicating that for the various cabinets in the kitchen, a total of 15 sensors were fitted to monitor activity. This information will be used to inform the pre-processing methodology for the sub activity data in subsequent analysis (**section XXX**). The sensor meta data set was inspected and stripped of special characters, and the sub activities requiring an input of energy (e.g., electrical, gas) were flagged with a boolean value, resulting in Table \ref{tab:TAB_sensorDataCleansedFinal}. The pre-processed sensor data was exported as `S1Sensors_preprocessed.csv`.
```{r TAB_sensorDataCleansedFinal, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1Sensors_preprocessed.csv')
head(df) %>% kable(format = "latex", caption = "Pre-processed sensor data table with addition columns for concatenated value, energy requirement and sub activity number", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```





## Importing & Preprocessing the Activities Data
The `S1activities.csv` dataset contains the collated activity and subactivity data from the work of Rockinson [@rockinsonActivityRecognitionHome]. The goal of pre-processing the `S1activities.csv` will be to restructure the dataset into a 'tidy' format, where the attributes are columns, the rows are instances, and each cell contains only one value (**REF Hadley**). Figure \ref{fig:s1actExcel} shows the data set open in a spreadsheet program and Table \ref{tab:TAB_rawDSS1} shows a sample of the dataset after importation of the dataset into the interactve development environment. Inspection of the original dataset via an interactive development environment shows a structure such that each 5 rows contains is one discrete set of data, for example:
* Row 1: Activity, Date, Start Time, End Time
* Row 2: Sub-activity (an action that can be executed as part of performing the activity) code-values
* Row 3: Sub-activity descriptive values
* Row 4: Sub-activity start time
* Row 5: Sub-activity end time

```{r s1actExcel, echo=FALSE, fig.align='center', fig.cap="The S1 Activities dataset shown in a spreadsheet program. The structure of the data, a mixture of tab delimited and comma delimited values with unequal row lengths, presents a variety of challenges with respect to pre-processing", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/s1actExcel.png')
```
```{python include=FALSE}
dsS1 = pd.read_csv(PATH + '/datasets/S1activities_data.csv', sep = 'delimiter', header = None, engine = 'python')
dsS1 = dsS1.loc[[5,6,7,8,9],:]
```
```{r TAB_rawDSS1, echo=FALSE, results='asis'}
head(py$dsS1) %>% kable(format = "latex", caption = "A sample of the S1 activities dataset after importation into the interactive development environment", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

Due to the varying number of comma-separated elements in each row (Figure \ref{fig:s1actExcel}) the `S1activities.csv` data was imported as an indexed dataframe, containing only one column, with 1475 rows. In each row, values were comma-separated. After initial imporation the dataframe was converted to a 2D array, using `np.array()`, where each row from the dataframe became an array within the 2D array. The 2D array was flattened to a 1D array using `.flatten()` and each group of observations was then chunked into a list of 5. Using the logic shown in Algorithm \ref{alg:actExtraction}, below, the activity, date and time data was extracted. During extraction in intermediate activities dataset was generated as seen below in Table \ref{tab:TAB_dsActIntermediate} followed by the tidy pre-processed activities dataset, as seen in Table \ref{tab:TAB_dsActFinal}.

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Intermediate dataframe with 'activity', 'date', 'startTime', 'endTime' as attributes}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{S1 Activities}
\Output{S1 Activities Intermediate}
\BlankLine
\Begin{
  array1-8 = [ ]\;
  i = 0\;
  \While{i < length(dataframe)}{
    array1.append(dataframe[i][0])\;
    array2.append([x.strip() for x in array1[i].split(',')])\;
    array3.append(array2[i][0])\;
    array4.append(array2[i][1])\;
    array5.append(array2[i][2])\;
    array6.append(array2[i][3])\;
    i = i + 1\;
    }
  dfIntermediate = pandas.DataFrame(list(zip(array3, array4, array5, array6)))\;
  start = (dfIntermediate.date + " " + dfIntermediate.startTime)\;
  end = (dfIntermediate.date + " " + dfIntermediate.endTime)\;
  i = 0\;
  \While{i < length(start)}{
    array7.append(datetime.strptime(start[i], mm/dd/yyyy HH:MM:SS))\;
    array8.append(datetime.strptime(end[i], mm/dd/yyyy HH:MM:SS))\;
    i = i + 1\;
    }
  dfFinal = pandas.DataFrame(list(zip(array3, array7, array8)))\;
  \KwRet dfIntermediate\;
  \KwRet dfFinal\;
}
\caption{Extraction of data from S1 Activities dataset}
\label{alg:actExtraction}
\end{algorithm} 

```{r TAB_dsActIntermediate, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1Activities_intermediate.csv')
head(df) %>% kable(format = "latex", caption = "The S1 Activities intermediate dataset", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

```{r TAB_dsActFinal, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1Activities_ds.csv')
head(df) %>% kable(format = "latex", caption = "The S1 activities tidy dataset", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

## Importing, Visualizing and Preprocessing the SubActivities Data
As mentioned above, the `S1Activities_data.csv` dataset contains the collated activity and subactivity data from the work of Rockinson [@rockinsonActivityRecognitionHome]. As with the activities data extraction, the dataframe was converted to a 2D array, using `np.array()`, where each row from the dataframe became an array within the 2D array. The 2D array was flattened to a 1D array using `.flatten()` and each group of observations was then chunked into a list of 5. The sub-activity, date and time data was then extracted from the dataset, by employing the logic shown in Algorithm \ref{alg:subActExtraction}, below.An intermediate sub-activities dataset was generated as seen below in Table \ref{tab:TAB_dsSUBIntermediate} followed by the tidy pre-processed sub-activities dataset, as seen in Table \ref{tab:TAB_dsSUBActFinal}. This dataset was exported as `S1SubActivities_ds.csv`.

\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Intermediate dataframe with 'activity', 'date', 'startTime', 'endTime' as attributes}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{S1 Activities}
\Output{S1 Activities Intermediate}
\BlankLine
\Begin{
  array1-21 = [ ]\;
  
  \While{i < length(dataframe)}{
    array1.append(dataframe[i][0])\;
    array2.append([x.strip() for x in array1[i].split(',')])\;
    array3.append(array2[i][0])\;
    array4.append(array2[i][1])\;
    array5.append(array2[i][2])\;
    array6.append(array2[i][3])\;
    i = i + 1\;
    }
  \While{i < length(dataframe)}{
    array7.append(dataframe[i][1])\;
    array8.append([x.strip() for x in array7[i].split(',')])\;
    array9.append(dataframe[i][2])\;
    array10.append([x.strip() for x in array9[i].split(',')])\;
    array11.append(dataframe[i][3])\;
    array12.append([x.strip() for x in array11[i].split(',')])\;
    array13.append(dataframe[i][4])\;
    array14.append([x.strip() for x in array13[i].split(',')])\;
    i = i + 1\;  
    }
  \While{i < length(dataframe)}{
    for x in range(len(array8[i])) : array15.append(array4[i])\;
    i = i + 1\;  
    }
  for sublist in array8: for item in sublist: array16.append(item)\;
  for sublist in array10: for item in sublist: array17.append(item)\;
  for sublist in array12: for item in sublist: array18.append(item)\;
  for sublist in array13: for item in sublist: array19.append(item)\;
  dfIntermediate = pandas.DataFrame(list(zip(array16, array17, array15, array18, array19)))\;
  start = (dfIntermediate.date + " " + dfIntermediate.startTime)\;
  end = (dfIntermediate.date + " " + dfIntermediate.endTime)\;
  \While{i < length(start)}{
    array20.append(datetime.strptime(start[i], mm/dd/yyyy HH:MM:SS))\;
    array21.append(datetime.strptime(end[i], mm/dd/yyyy HH:MM:SS))\;
    i = i + 1\;
    }
  dfFinal = pandas.DataFrame(list(zip(array16, array17, array20, array21)))\;
  \KwRet dfIntermediate\;
  \KwRet dfFinal\;
}
\caption{Extraction of sub-activity data from S1 activities dataset}
\label{alg:subActExtraction}
\end{algorithm} 

\pagebreak

```{r TAB_dsSUBIntermediate, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubActivities_intermediate.csv')
head(df) %>% kable(format = "latex", caption = "The S1 sub-activities intermediate dataset", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```
```{r TAB_dsSUBActFinal, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubActivities_ds.csv')
head(df) %>% kable(format = "latex", caption = "The S1 sub-activities tidy dataset", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

### Amalgamating 'duplicate' sub-activity instances

As mentioned in _Importing & Preprocessing the Sensor Meta Data_, 10 of the sensors have duplicated values, owing to the presence of more than one sensor for certain activities. The sub-activity `kitchen_refrigerator`, for example, has the numbers [91, 126, 144] associated with it, as seen in Table \ref{tab:TAB_fridge}, below. These values are subsequently replaced with [126]. As is the case with all sub-activities with degenerate numbers, the end value is chosen based on the first appearance in the S1 Sensors dataset.

All duplicated values, as seen in Table \ref{tab:TAB_numDupes} were aggregated to have the same sensor ID / subAct number, thus giving a value of n=1 with respect to duplication.The first occurence of each degenerate subactivity provided the subAct number to be used for all subsequent instances. This dataset was exported as `S1SubActivities_preprocessed.csv`. **It was found these data has dimensionality of ___**

```{r TAB_fridge, echo=FALSE, results='asis'}
fridge <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubActivities_fridge.csv')
head(fridge, n=5) %>% kable(format = "latex", caption = "Example of duplicate sub-activity number removal", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped", "scale_down"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8) %>% add_header_above(c("Prior to duplicate removal" = 4, "Post to duplicate removal" = 4))
```


### Addition of Temporal Features to the pre-processed sub-activities data

As noted by Basu et al, human behavior with respect to home appliance interaction and activity, is typically periodic in nature [@kaustavbasuApplianceUsagePrediction2012], that is, certain activities occur at specific times of day or in a particular sequence. It is helpful therefore to add categorical temporal features to our dataset, in order to inform further analysis. Using the Python `pandas` and `datetime` packages, the following features were added to all instances of the pre-processed S1 sub activities dataset; `dayNumeric`, `DAY`, `WDWE` (where WD is weekday and WE is weekend), `HOUR` (based on 24-hour time) and `durationSec` (the delta seconds value between `start` and `end`). This dataset was exported as `S1SubActivities_preprocessedWfeatures.csv`.

```{r TAB_ALL, echo=FALSE, message=FALSE, warning=FALSE}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubActivities_preprocessedWfeatures.csv', 
               col_types = cols(subActNum = col_character(), dayNumeric = col_character(), HOUR = col_character()))
dfNoDupes = df[!duplicated(df$subActNum),]
dfNoDupes %>% kable(format = "latex", caption = "A sample of n=1 of each sub-activity from the pre-processed dataset", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped", "scale_down"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
  #column_spec(c(1,2)) %>%
  #row_spec(c(27,29,34,35,36,38), background = "#CEF5E7") %>%
  #row_spec(c(1,4,5,6,7,8,9,10,11,12,15,17,19,20,21,23,33), background = "#C6CDF2") %>%
  #row_spec(c(3,13,14,16,18,22,24,25,26,28,30,31,32,37), background = "#EACBF2") %>%
  #row_spec(2, background = "#FAF8B9")
```


### SubActivity Visualisation and Outlier Inspection

#### Aggregated Line Chart
The `S1SubActivities_preprocessedWfeatures.csv` dataset was analysed using data visualisation techniques. A plot of aggregated sub-activity count versus sub-activity was generated, as seen in Figure \ref{fig:FIG_aggLineChart}, below. The chart shows that there is a high level of consistency with respect to the overall aggregated count of each sub-activity, when compared to the feature `Day of the Week`. **This is helpful for subsequent analysis.**

```{r message=FALSE, warning=FALSE, include=FALSE}
df$start <- ymd_hms(df$start)
df$start <- force_tz(df$start, "America/New_York")
df$end <- ymd_hms(df$end)
df$end <- force_tz(df$end, "America/New_York")
df$subAct <- as.factor(df$subAct)
total_counts <- df %>% group_by(DAY, subAct) %>% summarise(count=n())
```

```{r FIG_aggLineChart, echo=FALSE, fig.align='center', fig.cap="ADD Text", fig.pos='H', fig.height=5, fig.width=7, message=FALSE, warning=FALSE}
p <- ggplot(total_counts, aes(x = subAct, y = count, group = DAY, color = DAY))
p <- p + geom_line(size=0.5) + geom_point(size = 0.5) + theme_grey()
p <- p + coord_flip() + labs(title = "Plot of aggregated sub-activity count versus sub-activity",
                             x = "Sub-activity",
                             y = "Aggregated count") +
  theme(text = element_text(size=8), 
      axis.title.y=element_blank(),
      legend.title = element_blank(),
      legend.position = c(0.9,0.82),
      legend.background = element_rect(fill=alpha('white', 0.5)),
      legend.key.size = unit(5, "mm"))
p
```

#### Aggregated Box Plot of Sub-Activities

Figure \ref{fig:FIG_allBoxPlots}, below, shows boxplots of all sub-activities versus their duration values. In this plot, we can see many outliers, which will inform further analysis. We also note that many of the outliers are extremely unrealistic. Of particular concern Kitchen_Toaster with a duration of almost 30,000 seconds, Bathroom_toiletflush distributed up to the range of 20,000 seconds. This potentially indicates challenges with the initial data collection methodology or experimental conditions / setup. Each sub-activity will be individually considered with respect to it's outliers.

```{r FIG_allBoxPlots, echo=FALSE, fig.align='center', fig.cap="ADD Text", fig.height=5, fig.pos='H', fig.width=7, message=FALSE, warning=FALSE}
p <- ggplot(df, aes(subAct, durationSec))
p <- p + geom_boxplot(outlier.alpha = 0.8, outlier.size = 0.8) + # aes(colour = WDWE) #
  coord_flip() + theme_grey() +
  labs(title = "Box Plots",
       x = "SubActivity",
       y = "Duration (sec)") + 
  theme(text = element_text(size=8),
        axis.title.y=element_blank())
p
```

### Sub-Activity Cleansing - Outlier Removal

Via the inspection of each sub-activity the following set of instances were identified, as seen in Table \ref{tab:outlierSTATS}. In all cases the standard deviation of each aggregated instance (n=Count) is found to be greater than the mean, indicating that the data points are not even distributed with respect to total duration of the sub-activity, additionally, the median in all cases is found to be vastly different from the mean. The bathroom medicine cabinet, for example, has a mean value of 3188 seconds (53 minutes) and a median value of 88 seconds. Accross all sub-activities seen in Table \ref{tab:outlierSTATS}, the median gives more reasonable values. Therefore the median value for each sub-activity below will be used to fill the outlier values. There were in total 4 pre-processing methodologies used. 1. Filling outliers with median 2. All values replaced with one value 3. No further processing required 4. Sub-activity dropped

```{r outlierSTATS, echo=FALSE, message=FALSE, warning=FALSE}
check <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubActivities_medianValues.csv')
check %>% kable(format = "latex", caption = "Summary table of outliers with respect to sub-activity duration over all instances", booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

### Sub-Activity Cleansing - 1) Filling outliers with median

An example of this methodology can be seen with the bathroom cabinet. This methodology was also applied to all of the other sub-activities in Table \ref{tab:outlierSTATS}.

#### Bathroom Cabinet - Sub-Activity 67
As per Table \ref{tab:outlierSTATS}, above, the bathroom cabinet has an overall count of n=104 in the dataset. In Figure \ref{fig:subAct67} the boxplot (A) in row one shows several outliers, most notably around the 20,000 second mark for Friday and the 25,000 mark for Sunday. When considering all weekday and weekend values together, the mean is found to be 460.22 seconds, while the standard deviation is found to be 3176.16. As the SD in this case greatly overwhelms the mean, and the values of 20,000 seconds and 25,000 seconds are highly unrealistic for the sub-activity in question, the outliers will be filled with the median (4.0 seconds). The resultant plots can be seen in the second row.

```{r subAct67, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the bathroom cabinet sub-activity.", fig.pos='H', out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct67.png')
```

#### Bathroom Medicine Cabinet - Sub-Activity 57
The bathroom medicine cabinet has an overall count of n=194 in the dataset. In Figure \ref{fig:subAct57} boxplot (A) in row one shows a large amount of outliers, ranging all the way to nearly 40,000 seconds (over 11 hours). This is clearly outside the expected range for this sub-activity, with a median of 88.0 seconds, a mean of 460.22 seconds, and a standard deviation of 7354.14 seconds. These outliers will be filled with the median (88.0 seconds). The resultant plots can be seen in the second row.
```{r subAct57, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bathroom Medicine Cabinet sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct57.png')
```

#### Study Drawer - Sub-Activity 82
The study drawer has an overall count of n=45 in the dataset. In Figure \ref{fig:subAct82} boxplot (A) in row one shows some abnormal results for Monday, Friday and Saturday. While the median is 6.0 seconds, there are results for Saturday that stretch beyond 30,000 seconds, clearly indicating sensor or measurement error. It is also outside the expected range for this sub-activity, causing the mean to be 1634.49 seconds with a standard deviation of 5432.41 seconds. These outliers will be filled with the median (6.0 seconds). The resultant plots can be seen in the second row.
```{r subAct82, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Study Drawer sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct82.png')
```

#### Bedroom Drawer - Sub-Activity 146
The bedroom drawer has an overall count of n=99. In Figure \ref{fig:subAct146} boxplot (A) in row one shows two main outliers, one on Friday at just under 10,000 seconds, and one on Wednesday at around 17,000 seconds. The median for this sub-activity is 10.0 seconds, and such outliers are clearly outside the expected range for this sub-activity, causing the mean to be 273.47 seconds with a standard deviation of 1918.73 seconds. These outliers will be filled with the median (10.0 seconds). The resultant plots can be seen in the second row.
```{r subAct146, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bedroom Drawer sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct146.png')
```

#### Kitchen Cabinet - Sub-Activity 132
The kitchen cabinet has an overall count of n=406. Figure \ref{fig:subAct132} boxplot (A) in row one shows one main outlier on Tuesday, at nearly 40,000 seconds. The median for this sub-activity is 7.0 seconds, and this outlier is clearly outside the expected range for this sub-activity, causing the mean to be 112.82 seconds with a standard deviation of 1864.01 seconds. This outlier will be filled with the median (7.0 seconds). The resultant plots can be seen in the second row.
```{r subAct132, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Cabinet sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct132.png')
```

#### Kitchen Microwave - Sub-Activity 143
The kitchen microwave has an overall count of n=61.Figure \ref{fig:subAct143} boxplot (A) in row one shows one main outlier on Thursday, at nearly 22,500 seconds. The median for this sub-activity is 6.0 seconds, and this outlier is clearly outside the expected range for this sub-activity, causing the mean to be 377.41 seconds with a standard deviation of 2868.67 seconds. This outlier will be filled with the median (6.0 seconds). The resultant plots can be seen in the second row.
```{r subAct143, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Microwave sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct143.png')
```

#### Kitchen Door - Sub-Activity 141
The kitchen door has an overall count of n=134. Figure \ref{fig:subAct141} boxplot (A) in row one shows several key outliers, notably one on Friday at over 1,750 seconds. The median for this sub-activity is 4.0 seconds, and these outliers are clearly outside the expected range for this sub-activity, causing the mean to be 46.11 seconds with a standard deviation of 174.19 seconds. These outliers will be filled with the median (4.0 seconds). The resultant plots can be seen in the second row.
```{r subAct141, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Door sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct141.png')
```

#### Bathroom Shower Faucet - Sub-Activity 93
The bathroom shower faucet has an overall count of n=88. Figure \ref{fig:subAct93} boxplot (A) in row one shows several outliers, with six of particular concern that range from over 10,000 seconds to 40,000 seconds. The median for this sub-activity is 15.0 seconds, and these extreme outliers are clearly outside the expected range for this sub-activity, causing the mean to be 1754.35 seconds with a standard deviation of 6540.73 seconds. The outliers will be filled with the median (15.0 seconds). The resultant plots can be seen in the second row.
```{r subAct93, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bathroom Shower Faucet sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct93.png')
```

#### Kitchen Drawer - Sub-Activity 125
The kitchen drawer has an overall count of n=208. Figure \ref{fig:subAct125} boxplot (A) in row one shows one main outlier on Sunday, at nearly 25,000 seconds. The median for this sub-activity is 4.0 seconds, and this outlier is clearly outside the expected range for this sub-activity, causing the mean to be 145.06 seconds with a standard deviation of 1727.58 seconds. This outlier will be filled with the median (4.0 seconds). The resultant plots can be seen in the second row.
```{r subAct125, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Drawer sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct125.png')
```

#### Kitchen Dishwasher - Sub-Activity 70
The kitchen dishwasher has an overall count of n=86. Figure \ref{fig:subAct70} boxplot (A) in row one shows several outliers, particularly six in the range of 7500 seconds to nearly 25,000 seconds. The observations for Thursdays are also much higher on average than the observations for the other days. The median for this sub-activity is 63.5 seconds, and these outliers and abnormal results are clearly outside the expected range for this sub-activity, causing the mean to be 1518.41 seconds with a standard deviation of 4146.64 seconds. These outliers will be filled with the median (63.5 seconds). The resultant plots can be seen in the second row.
```{r subAct70, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Dishwasher sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct70.png')
```

#### Bathroom Sink Faucet (Hot) - Sub-Activity 68
The bathroom sink faucet has an overall count of n=169. Figure \ref{fig:subAct68} boxplot (A) in row one shows one main outlier on Saturday, at over 6000 seconds. The median for this sub-activity is 11.0 seconds, and this outlier is clearly outside the expected range for this sub-activity, causing the mean to be 57.91 seconds with a standard deviation of 501.27 seconds. This outlier will be filled with the median (11.0 seconds). The resultant plots can be seen in the second row.
```{r subAct68, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bathroom Sink Faucet sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct68.png')
```

#### Kitchen Freezer - Sub-Activity 137
The kitchen freezer has an overall count of n=130. Figure \ref{fig:subAct137} boxplot (A) in row one shows many outliers on all days but Monday, and abnormal results on Monday compared to the other days. The median for this sub-activity is 39.0 seconds, and these outliers are clearly outside the expected range for this sub-activity, causing the mean to be 1738.11 seconds with a standard deviation of 4512.57 seconds. These outliers will be filled with the median (39.0 seconds). The resultant plots can be seen in the second row.
```{r subAct137, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Freezer sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct137.png')
```

#### Kitchen Lightswitch - Sub-Activity 105
The kitchen lightswitch has an overall count of n=32. Figure \ref{fig:subAct105} boxplot (A) in row one shows many abnormal results, including one observation on Sunday at nearly 55,000 seconds, and abnormally high results for Saturday when compared to the other days. The median for this sub-activity is 4454.0 seconds, and many of these results and outliers are clearly outside the expected range for this sub-activity, causing the mean to be 9318.72 seconds with a standard deviation of 13,081.34 seconds. The outliers will be filled with the median (4454.0 seconds). The resultant plots can be seen in the second row.
```{r subAct105, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Lightswitch sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct105.png')
```

#### Study Lightswitch - Sub-Activity 92
The study lightswitch has an overall count of n=26. Figure \ref{fig:subAct92} boxplot (A) in row one shows abnormal results for Saturday and Sunday as compared to the other days. The median for this sub-activity is 1728.5 seconds, and these abnormal results are clearly outside the expected range for this sub-activity, causing the mean to be 6430.73 seconds with a standard deviation of 11,002.30 seconds. These outliers will be filled with the median (1728.50 seconds). The resultant plots can be seen in the second row.
```{r subAct92, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Study Lightswitch sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct92.png')
```

#### Bathroom Door - Sub-Activity 130
The bathroom door has an overall count of n=73. Figure \ref{fig:subAct130} boxplot (A)  in row one shows six extreme outliers, with two of particular concern – one on Thursday at around 10,000 seconds and one on Sunday at around 17,500 seconds. The median for this sub-activity is 17.0 seconds, and these outliers are clearly outside the expected range for this sub-activity, causing the mean to be 461.62 seconds with a standard deviation of 2310.81 seconds. These outliers will be filled with the median (17.0 seconds). The resultant plots can be seen in the second row.
```{r subAct130, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bathroom Door sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct130.png')
```

#### Kitchen Toaster - Sub-Activity 131
The kitchen toaster has an overall count of n=71. Figure \ref{fig:subAct131} boxplot (A) in row one shows two main extreme outliers, one on Thursday at nearly 10,000 seconds and one on Friday at nearly 30,000 seconds. There are also general abnormal results on Friday as compared to the other days. The median for this sub-activity is 5.0 seconds, and these outliers and abnormal results are clearly outside the expected range for this sub-activity, causing the mean to be 790.37 seconds with a standard deviation of 3793.67 seconds. These outliers and abnormal results will be filled with the median (5.0 seconds). The resultant plots can be seen in the second row.
```{r subAct131, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Toaster sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct131.png')
```

#### Livingroom Lightswitch - Sub-Activity 107
The livingroom lightswitch has an overall count of n=8. Figure \ref{fig:subAct107} boxplot (A) in row one shows variable results due to the low count, with abnormally high results for Friday as compared to the other days. The median for this sub-activity is 6352.5 seconds, with a mean of 12,545.13 seconds and a standard deviation of 15,556.66 seconds. The abnormal values will be filled with the median (6352.50 seconds). The resultant plots can be seen in the second row.
```{r subAct107, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Livingroom Lightswitch sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct107.png')
```

### Foyer Closet - Sub-Activity 81
The foyer closer has an overall count of n=29 in the dataset. In Figure \ref{fig:subAct81} boxplot (A) in row one shows one key outlier just under 1000 seconds. When considering all values together, the mean is found to be 42.45 seconds, while the standard deviation is found to be 174.43 second. As the observation is so far from the median, it is clearly having a large impact on the mean and standard deviation, and the value of nearly 1000 seconds is unrealistic for the sub-activity in question, the outlier will be filled with the median (8.0 seconds). The resultant plots can be seen in the second row.
```{r subAct81, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Foyer Closet sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct81.png')
```


### SubActivity Cleansing - 2) All values replaced with one value

This methodology was applied only to the bathroom toiletflush, with all values being set to 1 second.

#### Bathroom Toiletflush - Sub-Activity 100
Given the domain knowledge that it is unrealistic for a toilet flush to last for thousands of seconds, and the lack of variability of the length of toilet flushes, it was appropriate in this circumstance to presume sensor or measurement error, and to replace the values instead with one value – 1 second. The initial (row one) and resultant (row two) plots can be seen in Figure \ref{fig:subAct100}

```{r subAct100, echo=FALSE, fig.cap="Initial and processed data box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the bathroom toiletflush sub-activity.", fig.pos='H', out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct100.png')
```

###  SubActivity Cleansing Review - 3) No futher processing required

Based on the investigation of the box plots (A in all sub activity plots), heat maps of day of week versus duration in seconds mapped by count (B in all subactivity plots) and heat maps of day of week versus hour of day mapped by count (C in all subactivity plots) the following sub activities were left in their native state; `bathroom_lightswitch` (Figure \ref{fig:subAct101}, below), `kitchen_refrigerator` (Figure \ref{fig:subAct126}, below), `bathroom_sinkfaucet-cold` (Figure \ref{fig:subAct88}, below), `foyer_door` (Figure \ref{fig:subAct140}, below), `kitchen_burner` (Figure \ref{fig:subAct140}, below), `foyer_lightswitch` (Figure \ref{fig:subAct104}, below), `bathroom_exhaustfan` (Figure \ref{fig:subAct96}, below), `bedroom_lightswitch` (Figure \ref{fig:subAct108}, below), `kitchen_oven` (Figure \ref{fig:subAct129}, below), `livingroom_lamp` (Figure \ref{fig:subAct76}, below), `kitchen_washingmachine` (Figure \ref{fig:subAct142}, below), `kitchen_laundrydryer` (Figure \ref{fig:subAct90}, below), `kitchen_garbagedisposal` (Figure \ref{fig:subAct98}, below) and `kitchen_coffeemachine` (Figure \ref{fig:subAct119}, below).

#### Bathroom Lightswitch - Sub-Activity 101
```{r subAct101, echo=FALSE, fig.cap="A caption", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct101.png')
```

#### Kitchen Refrigerator - Sub-Activity 126
```{r subAct126, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Refrigerator sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct126.png')
```

#### Bathroom Sink Faucet (Cold) - Sub-Activity 88
```{r subAct88, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bathroom sick faucted - cold sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct88.png')
```

#### Foyer Door - Sub-Activity 140
```{r subAct140, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Foyer Door sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct140.png')
```

#### Kitchen Burner - Sub-Activity 140
```{r subAct106, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Burner sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct106.png')
```

#### Foyer Lightswitch - Sub-Activity 104
```{r subAct104, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Foyer Lightswitch sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct104.png')
```

#### Bathroom Exhaust Fan - Sub-Activity 96
```{r subAct96, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bathroom Exhaust Fan sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct96.png')
```

#### Bedroom Lightswitch  - Sub-Activity 108
```{r subAct108, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bedroom Lightswitch sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct108.png')
```

#### Kitchen Oven  - Sub-Activity 129
```{r subAct129, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Oven sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct129.png')
```

#### Livingroom Lamp - Sub-Activity 76
```{r subAct76, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Livingroom Lamp sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct76.png')
```

#### Kitchen Washingmachine - Sub-Activity 142
```{r subAct142, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Washingmachine sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct142.png')
```

#### Kitchen Laundry Dryer - Sub-Activity 90
```{r subAct90, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Laundry Dryer sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct90.png')
```

#### Kitchen Garbage Disposal - Sub-Activity 98
```{r subAct98, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Garbage Disposal sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct98.png')
```

#### Kitchen Coffee Machine - Sub-Activity 119
```{r subAct119, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Coffee Machine sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct119.png')
```

###  SubActivity Cleansing - 4) Sub-activity dropped

Based on the investigation of the box plots (A in all sub activity plots), heat maps of day of week versus duration in seconds mapped by count (B in all subactivity plots) and heat maps of day of week versus hour of day mapped by count (C in all subactivity plots) the following sub activities were dropped from the dataset. This livingroom DVD sub activity, as plotted below in Figure \ref{fig:subAct56} was dropped from the dataset due to it's sparsity (only two instances in the dataset). Similarly, the bedroom lamp sub-activity, as plotted below in Figure \ref{fig:subAct64} also had only two instances in the dataset and was dropped. The bedroom jewelrybox subactivity, as seen in Figure \ref{fig:subAct139} was dropped due to the extremely specific nature of this subactivity. In building a generalized model, we do not wish to have such datapoints present for our analysis. Likewise with the bedroom jewelry box, the kitchen cereal subactivity, Figure \ref{fig:subAct145} below, were dropped for the same reason.

#### Livingroom DVD - Sub-Activity 56
```{r subAct56, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Livingroom DVD sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct56.png')
```

#### Bedroom Lamp - Sub-Activity 64
```{r subAct64, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bedroom Lamp sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct64.png')
```

#### Bedroom Jewelrybox - Sub-Activity 139
```{r subAct139, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Bedroom Jewelrybox sub activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct139.png')
```

#### Kitchen Cereal - Sub-Activity 145
```{r subAct145, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Cereal sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct145.png')
```

#### Kitchen Containers - Sub-Activity 60
```{r subAct60, echo=FALSE, fig.cap="Box plot (A) and heat maps (B = day of week versus duration in seconds mapped by count, C = day of week versus hour of day mapped by count) for the Kitchen Containers sub-activity.", out.width = '100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/subAct60.png')
```

###  The Final pre-processed dataset
The final pre-processed dataset had the following attributes; `subActNum` (the aub-activity number from the original `S1 Activities` dataset), `subAct` (the aub-activity descriptor from the original `S1 Activities` dataset), `start` (the start datetime for this specific instance of the sub-activity), `end` (the end datetime for this specific instance of the aub-activity), `dayNumeric` (a categorical variable to indiate day), `DAY` (e.g., 'Fri', 'Sat'), `WDWE` (WD signifies 'weekday' and WE signifies 'weekend'), `HOUR` (a categorical value to indicate time of the day, by the hour). And a dimensionality of [**DIM**].

## Data Analysis and Manipulation

The primary focus of this section will be qualitative and quantitative analysis of the pre-processed data. This includes casting the data into a structure for sequential analysis via Sankey diagrams, and into a Boolean Array structure for subsequent machine learning.

### Sequential Analysis Algorithm

An algorithm was created to capture the amount of time between the start of one event (`Event A`) and the start of another event (`Event B`). In is worth noting that as the target dataset (Table \ref{tab:}) is row-wise event data on a continuous scale, without correct bounds, this algorithm would consider the start of every `Event B` after each `Event A`, subsequently creating an extremely noisy and large dataset via a combinatorial explosion. In this scenario, we would have meaningless associations with delta values in the order of hundreds of thousands of seconds.

The algorithm `id_delta` was created to perform sequential analysis without combinatorial explosion and to capture the amount of time between the start of `Event A` and the start of event `Event B`, as the value `Delta` in seconds. The algorithm does this by taking three arguments. The first argument, `events`, is the dataset with each event occuring as a row instance. The second argument, `n`, is the total number of subsequent `Event B` for any one `Event A`. The final argument, `delta_threshold` takes interger quantities representative of the number of seconds into the future to consider. 

By way of an example `id_delta(ds, 10, dt.timedelta(0,-60))` for each event row instance in `ds`, `10` neighbouring subevents will be captured, as long as they occur within a range of `0` till `60` seconds forward. In this scenario, if only four `Event B` occur within sixty seconds of `EventA` then only these four will be captured. Likewise, if thirteen `EventB` occur within sixty seconds of `EventA`, then only ten will be captured.

```python
def id_delta(events, n=1, delta_threshold=dt.timedelta(-99)): # D
    nns = [] # Create an empty array
    for row in events.itertuples():
        start_time = getattr(row, 'start') # Extract `start` from data set
        end_time = getattr(row, 'end') # Extract `end` from data set
        subActNum = getattr(row, 'subActNum') # Extract `subActNum` from data set
        row_index = getattr(row, 'Index') # Extract `index` from data set
        
        nn = events[(events.start >= start_time) & # 
                    (events.index != row_index) & # 
                    ((start_time - events.start) > delta_threshold)][:n]
        ordered = pd.DataFrame()
        ordered['Dummy'] = nn['subActNum'] # Created for indexing purposes
        ordered['EventA'] = subActNum
        ordered['EventB'] = nn['subActNum']
        ordered['EvA_Start'] = start_time
        ordered['EvB_Start'] = nn['start']
        ordered['EvA_End'] = end_time
        ordered['EvB_End'] = nn['end']
        del ordered['Dummy'] # Dropped
        nns.append(ordered)
  
    result = pd.concat(nns)
    result['Delta'] = np.where(result['EvA_Start'] == result['EvB_Start'], 0,
                               (result['EvB_Start'] - result['EvA_Start']))
    result['Delta'] = result['Delta'].dt.total_seconds()
    return result
```

A shotgun analysis of the combinatorics associated with varying input arguments resulted in the following observations; 
* One neighbour for 10 seconds gave a dataset of 1063 instances (`id_delta(ds, 1, dt.timedelta(0,-10))`)
* One neighbour for 60 seconds gave a dataset of 1989 instances (`id_delta(ds, 1, dt.timedelta(0,-60))`)
* Ten neighbours for 10 seconds gave a dataset of 1536 instances (`id_delta(ds, 10, dt.timedelta(0,-10))`)
* Ten neighbours for 60 seconds gave a dataset of 5385 instances (`id_delta(ds, 10, dt.timedelta(0,-60))`)

We are focussing our analysis on the sequential analysis algorithm run with the parameters of one neighbouring sub-activity over a sixty second period (`id_delta(ds, 1, dt.timedelta(0,-60))`), the head of the resultant dataset (`ds_1n_60s.csv`) can be Table \ref{tab:}, below. The reasoning behind chosing this dataset being 1) It is best practise to start with the most 'simple' form of a problem in order to develop understanding, in this case being that one neighbour only is considered for each row and 2) For further analysis these data may be collapsed from a seconds time scale to a minute time scale, so it is useful to know the variety of activities that can occur within one minute (60 seconds of each other). 

```{r TAB_ds_1n_60s, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/ds_1n_60s.csv')
head(df) %>% kable(format = "latex", caption = "Sequential analysis result: one neighbouring sub-activity over a sixty second period", 
                   booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped", "scale_down"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

### Qualitative Sequential Analysis via Sankey Diagram
In order to develop understanding of the interrelaterness of the sub-activites, the data from the sequential analysis `ds_1n_60s.csv` were plotted as Sankey diagrams using a bespoke wrapper method written in Python. Using the previously mentioned features of `HOUR` and `WDWE` in the dataset, the resultant diagrams were segmented over 6 time periods, namely:

1. Figure \ref{fig:sankey_WeekdayMorningds_1n_60s} - Weekday Morning, initial morning sub-activity until 11.59 am
2. Figure \ref{fig:sankey_WeekdayAfternoonds_1n_60s} - Weekday Afternoon, 12 noon until 17.59 pm
3. Figure \ref{fig:sankey_WeekdayEveningds_1n_60s} - Weekday Evening, 18:00 pm until mid-night
4. Figure \ref{fig:sankey_WeekendMorningds_1n_60s} - Weekend Morning, initial morning sub-activity until 11.59 am
5. Figure \ref{fig:sankey_WeekendAfternoonds_1n_60s} - Weekend Afternoon, 12 noon until 17.59pm
6. Figure \ref{fig:sankey_WeekendEveningds_1n_60s} - Weekend Evening, 18:00pm untill mid-night

Sankey diagrams are used to visualise flows of energy, materials or other resources in a variety of applications (Lupton, 2017).In our case, the Sankey diagrams will allow us to visualise the flow of end-user interaction with each sub-activity over the time-boxes 1 through 6, as described above. Our Sankey diagrams (derived from the 1 neighbour over 60 seconds sequential analysis) have the following features i) The each represent a specific aggregated time period from the dataset, ii) In each Sankey, the sub-acitivity nodes that require energy input only from the end user (e.g., Kitchen Door) are blue, while the sub-activity nodes that require external energy to function (e.g., Kitchen Toaster), iii) The width of each sub-activity in the Sankey diagram is indicative to the number of times interaction occured, a thicker node means more interaction, iv) The lines between each node indicate the event of one sub-activity proceeding or preceeding another and v) Some events nodes are centralised, with at least one activity 'flowing in' and at least one 'flowing out' while other nodes a terminal, with only activities 'flowing in' or 'flowing out' (but not both).

Each Sankey diagram, as seen below, shows a high level of interactivity between the various sub-acitivities. We observer that of all the 'no external energy' sub-activities, only the foyer closet (as seen in Figure \ref{fig:sankey_WeekdayAfternoonds_1n_60s} and Figure \ref{fig:sankey_WeekdayEveningds_1n_60s}) acts as a terminal node. In comparision, numerous sub-activities that require energy (as seen in Figure \ref{fig:sankey_WeekdayEveningds_1n_60s} and Figure \ref{fig:sankey_WeekendEveningds_1n_60s}) act as terminal nodes. We also observe that there is a very level of interactivity between the sub-activities that do and do no require an external energy source (red and blue nodes, respectively) to function. This will provide direction for further analyis.

#### Weekday Morning, initial morning sub-activity until 11.59 am
```{r sankey_WeekdayMorningds_1n_60s, echo=FALSE, fig.align='center', fig.cap="Sankey Diagram of Weekday Mornings, considering all one neighbour per instance within a 60 second period", out.width='100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/WeekdayMorningds_1n_60s.png')
```
#### Weekday Afternoon, 12 noon until 17.59 pm
```{r sankey_WeekdayAfternoonds_1n_60s, echo=FALSE, fig.align='center', fig.cap="Sankey Diagram of Weekday Afternoons, considering all one neighbour per instance within a 60 second period", out.width='100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/WeekdayAfternoonds_1n_60s.png')
```
#### Weekday Evening, 18:00 pm until mid-night
```{r sankey_WeekdayEveningds_1n_60s, echo=FALSE, fig.align='center', fig.cap="Sankey Diagram of Weekday Evenings, considering all one neighbour per instance within a 60 second period", out.width='100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/WeekdayEveningds_1n_60s.png')
```
#### Weekend Morning, initial morning sub-activity until 11.59 am
```{r sankey_WeekendMorningds_1n_60s, echo=FALSE, fig.align='center', fig.cap="Sankey Diagram of Weekend Mornings, considering all one neighbour per instance within a 60 second period", out.width='100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/WeekendMorningds_1n_60s.png')
```
#### Weekend Afternoon, 12 noon until 17.59pm
```{r sankey_WeekendAfternoonds_1n_60s, echo=FALSE, fig.align='center', fig.cap="Sankey Diagram of Weekend Afternoons, considering all one neighbour per instance within a 60 second period", out.width='100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/WeekendAfternoonds_1n_60s.png')
```
#### Weekend Evening, 18:00pm untill mid-night
```{r sankey_WeekendEveningds_1n_60s, echo=FALSE, fig.align='center', fig.cap="Sankey Diagram of Weekend Evenings, considering all one neighbour per instance within a 60 second period", out.width='100%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/WeekendEveningds_1n_60s.png')
```

### Re-casting the Dataset as a Boolean Array
Based on the analysis above, we sought a data structure whereby every simultaneous subactivity occuring at one point in time could potentially be evaluated. In order to acheive this without having to place a large emphasis on complex combinatorics, it was decided to cast the data structure into a Boolean Array. In this structure, each sub-activity will become an attribute, with every row is a timestamp index. This structure will span the duration of the dataset - in other words, every second that was accounted for in the original dataset will have a row instance (a row).

The required transformations were for performed on the pre-processed dataset, with an initial index occuring at 27/03/2003 6:43:40 am, and the final index occuring at 11/4/2003 10:24:18 pm. In some regions of the dataset there were no results, so it is not an uninterrupted timescale from the first to last index. This will not affect our analysis as we are considering sequences of events as they occur with respect to concurrency. 

A sample of the processed boolean array data (with omitted attributes) can be seen below in Table \ref{tab:TAB_S1SubAct_B_s_EXAMPLE}. Here we can see the following sub-activities in an 'active' state; 57 (bathroom medicine cabinet), 101 (bathroom lightswitch) and 96 (bathroom exhaustfan). All other sub-activities are inactive. At 01/04/2003 6:54:56 am sub-activity 88 (bathroom sinkfaucet - hot) becomes active, followed by one second later at 01/04/2003 6:54:57 am sub-activity 68 (bathroom sinkfaucet - cold) becoming active. Sub-activity 57, 101, 68, 88 and 96 all remain active at 01/04/2003 6:55:01 am. 

```{r TAB_S1SubAct_B_s_EXAMPLE, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubAct_B_s_EXAMPLE.csv')
df %>% kable(format = "latex", caption = "Add text", 
                   booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped", "scale_down"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

This data structure thus offers the following benefits; i) Simultaneous consideration of the state of multiple sub-activities,ii) Present in a format that is still readily human interprettable and iii) Present in a format that can be easily consumed by a machine.

And the following challenges; i) The dataset is overall very large, potentially requiring large amounts of computational power for downstream analysis, ii) The dataset is potentially highly homogeneous in certain temporal regions and iii) A second-by-second analysis is most likely to granular for the purposes of this work

In order to remedy this, we converted the Boolean Array structure into the minute-scale, by aggregating all the second values for each attribute for each minute. For example, if a sub-activity had occured for 30 seconds during one minute, it will not be represented as a '1' for that minute in the dataframe.

```{r TAB_S1SubAct_B_m_EXAMPLE, echo=FALSE, results='asis'}
df <- read_csv('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/intermediate_datasets/S1SubAct_B_m_EXAMPLE.csv')
df %>% kable(format = "latex", caption = "Add text", 
                   booktabs = T) %>% 
  kable_styling(latex_options = c("hold_position", "striped", "scale_down"),
                bootstrap_options = c("striped", "hover", "condensed"),
                font_size = 8)
```

# Machine Learning Analysis

Using the pre-processed boolean array data (minute-scale) we will now attempt to create a predictive model with the aim of identifying patterns of co-occurrence between the sub-activities. As we are attempting to predict the on or off state of activities, this is a binary classification problem. As discussed during the Sankey analysis, the sub-activities can be broadly categorised as requiring energy input only from the end user (e.g., Kitchen Door) or requiring external energy to function (e.g., Kitchen Toaster). Note that for the purposes of further discussion such appliances will be referred to as energy-intensive (e.g., light switch, fridge and so on). Our model we will only attempt to predict the state of energy-intensive appliances, for reasons outlined in the discussion section.

## The Machine Learning Algorithm
A wrapper method will be created centered around the the sklearn Decision Tree Classifier. This method will use a decision tree model and optimize its hyperparameters using a grid search. The grid search will be performed over split criterion ('gini' and 'entropy') and maximum depth. Feature selection will be performed using the entropy-based method of mutual information, for the top five features. Cross-validation will be performed using 5-folds. The wrapper method will enable the model to perform the functions described above in an iterative fashion for each of the energy-intensive sub-activity features in the dataset. The processed sub-activities meta data will also be parsed to provide the data on which sub-activity is energy intensive and which is not. Throughout the iterations, key data will be parsed to *.csv format for later analysis.

### Data
The dataset used will be the boolean array on the minute scale. These data currently have an datetime index, with dimensionality of **XXXX**. In preparation for the analysis, the datetime index is removed, as will be discussed below. Important to note that these data currently contain no categorical features (e.g, day of week, weekend / weekday, hour of day, room of house, e.t.c,).

```python
ds = pd.read_csv(PATH + '/intermediate_datasets/S1SubAct_B_m_NoDupes.csv', index_col = 'duration')
ds.reset_index(drop = True, inplace = True)
```
### The Wrapper Method
The following python script was used to perform the desired machine learning analysis.

```python
for subAct in poweredSubActs:
    print("BEGIN:", subAct)
    row = {"Target":subAct}
    Data = ds.drop(columns = subAct).values
    target = ds[subAct]
    
    D_train, D_test, t_train, t_test = train_test_split(Data, target, test_size = 0.3, 
                                        random_state=999)
    cv_method = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 3, random_state = 999)
    dt_classifier = DecisionTreeClassifier(random_state=999)
    params_DT = {'criterion': ['gini', 'entropy'], 
                'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10]}
    gs = GridSearchCV(estimator=dt_classifier, param_grid=params_DT, cv=cv_method,
                      verbose=1, scoring='accuracy')
    gs.fit(Data, target)
    
    print("best params:", gs.best_params_)
    print("best score:", gs.best_score_)
    
    row['Original_Fit'] = gs.best_score_
    num_features = 5
    
    fs_fit_mutual_info = fs.SelectKBest(fs.mutual_info_classif, k=num_features)
    fs_fit_mutual_info.fit_transform(Data, target)
    fs_indices_mutual_info = np.argsort(fs_fit_mutual_info.scores_)[::-1][0:num_features]
    
    best_features_mutual_info = ds.columns[fs_indices_mutual_info].values    
    feature_importances_mutual_info = fs_fit_mutual_info.scores_[fs_indices_mutual_info]
    
    results_DT = pd.DataFrame(gs.cv_results_['params'])
    results_DT['test_score'] = gs.cv_results_['mean_test_score']
    results_DT.to_csv(subAct + "_dt.csv", index=False)
    
    t_pred = gs.predict(D_test)
    t_prob = gs.predict_proba(D_test)
    
    metrics.roc_auc_score(t_test, t_pred)
    fpr, tpr, _ = metrics.roc_curve(t_test, t_prob[:, 1])
    roc_auc = metrics.auc(fpr, tpr)    
    
    print('roc_auc', roc_auc)
    df = pd.DataFrame({'fpr': fpr, 'tpr': tpr})
    df.to_csv(subAct + "_roc.csv", index = False)
    
    report = metrics.classification_report(t_test, t_pred, output_dict=True)
    rep = pd.DataFrame(report).transpose()
    rep.to_csv(subAct + "_rep.csv", index=True)
    print(metrics.confusion_matrix(t_test, t_pred))
    
    report = metrics.confusion_matrix(t_test, t_pred)
    rep = pd.DataFrame(report).transpose()
    rep.to_csv(subAct + "_confusion.csv", index=True)    
```
### Machine Learning Model Results
The following section provides an evaluation of the machine learning model results.






Test for in \ref{fig:fig1} we see XYZ

```{r echo=FALSE, fig.cap="ADD TEXT", fig.align='center', out.width = '80%'}
knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/iosMockUps.png')
```






# Discussion
unbalanced data... 
##
- Collapse 1 second gives one for a minute AND
- Collapse 59 seconds give one for a minute
- How much would such a thing affect the balance of the data?

## Energy only sub-activity

For our model, we aim to only predict the state (on or off) of the later category, that is, sub-activities that require an energy source to function. This is because, in the larger context of this work, only energy-based sub-activities could be (in theory) 

## Machine Learning Models
unbalanced data... 
- Feature selection will be performed using the entropy-based method of mutual information. USING MORE than 5 - 'the extremely low imporance value indicates that the overall importance of the features is low, so considering more than 5 would not be beneficial in find tuning our model.'
##







\pagebreak







Reduce image border (not working???)
https://holtzy.github.io/Pimp-my-rmd/ 

S1SubActivities_temporalFeaturesCLEANSED.csv = 'Final' dataset


#### Algorithm 1
Text
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgoLined
\KwResult{Write here the result}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Write here the input}
\Output{Write here the output}
\BlankLine
\While{While condition}{
    instructions\;
    \eIf{condition}{
        instructions1\;
        instructions2\;
    }{
        instructions3\;
    }
}
\caption{While loop with If/Else condition}
\end{algorithm} 

https://daattali.com/shiny/colourInput/

#### Strip Plots

```{r echo=FALSE, fig.cap="ADD TEXT", out.width = '60%'}
#knitr::include_graphics('/Users/alistairgj/Documents/GitHub/IoT_ResearchProject/IoT_November/images/strip1.png')
```

---
nocite: |
  @Svenssonguideornitho2007
  `r paste(packages_keys$value, collapse = "\n  ")`
---
---
nocite: |
  @article{owidextremepoverty,
      author = {Max Roser and Esteban Ortiz-Ospina},
      title = {Global Extreme Poverty},
      journal = {Our World in Data},
      year = {2019},
      note = {https://ourworldindata.org/extreme-poverty}
      }
---

# References




